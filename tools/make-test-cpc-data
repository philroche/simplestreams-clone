#!/usr/bin/python

import argparse
import copy
import errno
from Cheetah.Template import Template
import json
import os
import os.path
import subprocess
import sys
import yaml

CONTENT_EXT = ".yaml"

GET_QUERY = ("rsync -avz --delete --exclude \".bzr/*\" " +
             "cloud-images.ubuntu.com::uec-images/query/ ci-query")

# whitelist of releases
RELEASES = (
#    "hardy",
    "lucid",
    "oneiric",
    "precise",
    "quantal",
    "raring",
)

# whitelist of builds
BUILDS = ("server")

NUM_DAILIES = 4

REL2VER = {
    "hardy": {'version': "8.04", 'devname': "Hardy Heron"},
    "lucid": {'version': "10.04", 'devname': "Lucid Lynx"},
    "oneiric": {'version': "11.10", 'devname': "Oneiric Ocelot"},
    "precise": {'version': "12.04", 'devname': "Precise Pangolin"},
    "quantal": {'version': "12.10", 'devname': "Quantal Quetzal"},
    "raring": {'version': "13.04", 'devname': "Raring Ringtail"},
}

STREAM_TEMPLATE = """
description: Image ids for Ubuntu Cloud Ubuntu ${release.version} (${release.devname}) on ${cloud.name}/${region.name}
format: stream-1.0
item_groups:
#for $item_group in $item_groups:
 - serial: ${item_group.serial}
   label: ${item_group.label}
   items:
   #for $item in $item_group.items
    #if $item.vtype == "hvm"
    #set prefix="hvm/"
    #else if $item.root_store == "instance-store"
    #set prefix=""
    #else
    #set prefix=$item.root_store + "/"
    #end if
    #if $item_group.label == "daily":
    #set name="ubuntu-" + $release.name + "-daily-" + $arch.name + "-" + $build.name + "-" + $item_group.serial
    #else
    #set name="ubuntu-" + $release.name + "-" + $release.version + "-" + $arch.name + "-" + $build.name + "-" + $item_group.serial
    #end if
    - name: $name
      root_store: ${item.root_store}
      virt_type: ${item.vtype}
      image_id: ${item.id}
   #end for
#end for

tags:
  region: ${region.name}
  arch: ${arch.name}
  release: ${release.name}
  version: "${release.version}"
  stream: ${stream.name}
  cloud: ${cloud.name}
  region: ${region.name}
  endpoint: https://ec2.${region.name}.amazonaws.com
"""

COLLECTION_TEMPLATE = """
description: Ubuntu Certified Public Cloud image id streams
format: stream-collection:1.0
streams:
#for $stream in $streams:
 - url: $stream.url
 #for $tag in $stream.tags:
   $tag: $stream.tags[$tag]
 #end for
#end for

tags:
 #for tag in $tags:
 $tag: ${tags[tag]}
 #end for
"""

PGP_SIGNED_MESSAGE_HEADER = "-----BEGIN PGP SIGNED MESSAGE-----"
PGP_SIGNATURE_HEADER = "-----BEGIN PGP SIGNATURE-----"
PGP_SIGNATURE_FOOTER = "-----END PGP SIGNATURE-----"


def render_string(content, params):
    if not params:
        params = {}
    return Template(content, searchList=[params]).respond()


def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    return


def read_possibly_signed(path):
    content = ""
    with open(path, "r") as fp:
        content = fp.read()

    if content.startswith(PGP_SIGNED_MESSAGE_HEADER):
        # http://rfc-ref.org/RFC-TEXTS/2440/chapter7.html
        subprocess.check_output(["gpg", "--batch", "--verify", path],
                                stderr=subprocess.STDOUT)
        ret = {'body': '', 'signature': '', 'garbage': ''}
        signature = ""
        lines = content.splitlines()
        i = 0
        for i in range(0, len(lines)):
            if lines[i] == PGP_SIGNED_MESSAGE_HEADER:
                mode = "header"
                continue
            elif mode == "header":
                if lines[i] != "":
                    mode = "body"
                continue
            elif lines[i] == PGP_SIGNATURE_HEADER:
                mode = "signature"
                continue
            elif lines[i] == PGP_SIGNATURE_FOOTER:
                mode = "garbage"
                continue

            # dash-escaped content in body
            if lines[i].startswith("- ") and mode == "body":
                ret[mode] += lines[i][2:] + "\n"
            else:
                ret[mode] += lines[i] + "\n"

        return(ret['body'], ret['signature'])
    else:
        return(content, None)


def signfile(path):
    tmpfile = "%s.tmp" % path
    os.rename(path, tmpfile)
    subprocess.check_output(["gpg", "--batch", "--output", path,
                             "--clearsign", tmpfile])
    os.unlink(tmpfile)


def dumps(content):
    if CONTENT_EXT == ".yaml":
        return yaml.safe_dump(content)
    else:
        return json.dumps(content)


def load_content(path):
    (content, signature) = read_possibly_signed(path)
    if path.endswith(".yaml"):
        return yaml.safe_load(content)
    else:
        return json.loads(content)


def load_query(path, cloud="aws", tree=None):
    streams = [f[0:-len(".latest.txt")]
               for f in os.listdir("ci-query")
                   if f.endswith("latest.txt")]
    if tree is None:
        tree = {}

    # $stream/$cloud/$region/$buildname/$release/$arch
    # $stream/aws/$region/${buildname}/${release['name']}/${arch}"]
    for stream in streams:
        if stream not in tree:
            tree[stream] = {cloud: {}}
        id_files = []

        latest_f = "%s/%s.latest.txt" % (path, stream)

        # get the builds and releases
        with open(latest_f) as fp:
            for line in fp.readlines():
                (rel, build, _stream, _serial) = line.split("\t")

                if ((len(BUILDS) and build not in BUILDS) or
                    (len(RELEASES) and rel not in RELEASES)):
                    continue

                id_files.append("%s/%s/%s/%s.txt" %
                    (path, rel, build, stream))

        cdata = tree[stream][cloud]
        for id_file in id_files:
            lines = reversed(open(id_file).readlines())
            for line in lines:
                line = line.rstrip("\n\r") + "\tBOGUS"
                (rel, build, label, serial, store, arch, region,
                 iid, _kern, _rmd, vtype, _bogus) = line.split("\t", 11)

                if region not in cdata:
                    cdata[region] = {}
                if build not in cdata[region]:
                    cdata[region][build] = {}
                if rel not in cdata[region][build]:
                    cdata[region][build][rel] = {}
                if arch not in cdata[region][build][rel]:
                    cdata[region][build][rel][arch] = []

                item_groups = cdata[region][build][rel][arch]

                item_group = None
                for ig in item_groups:
                    if ig['serial'] == serial:
                        item_group = ig

                if not item_group:
                    # check NUM_DAILIES
                    if (stream == "daily" and
                        len(item_groups) == NUM_DAILIES):
                        break
                    item_group = {'serial': serial, 'items': []}
                    item_groups.append(item_group)

                # item groups (same serial) all have the same label
                item_group['label'] = label
                cur_item = {'root_store': store,
                            'vtype': vtype, 'id': iid}
                item_group['items'].append(cur_item)

    return tree


def process(cur, data, level, layout, callback, passthrough):
    for item in cur:
        if isinstance(item, dict):
            data[layout[level]['name']] = item.copy()
        else:
            data[layout[level]['name']] = {'name': item}

        curdatum = data[layout[level]['name']]

        if callable(layout[level].get('populate', None)):
            layout[level]['populate'](curdatum)

        if (level + 1) == len(layout):
            path = '/'.join([data[n['name']]['name'] for n in layout])
            callback(cur[item], data, path, passthrough)
        else:
            process(cur[item], data, level + 1, layout, callback,
                    passthrough)


def name2ver(indict):
    indict.update(REL2VER[indict['name']])


def writer(item, data, path, passthrough):
    params = data.copy()
    params['item_groups'] = item
    url = "%s.yaml" % path
    fname = "%s/%s" % (passthrough['output_d'], url)
    mkdir_p(os.path.dirname(fname))
    open(fname, "w").write(render_string(passthrough['template'], params))
    passthrough['streams'].append(url)
    sys.stderr.write(fname + "\n")


def main():
    parser = argparse.ArgumentParser(description="create example content tree")

    parser.add_argument("query_tree", metavar='query_tree',
                        help=('read in content from /query tree. Hint: ' +
                              GET_QUERY))

    parser.add_argument("out_d", metavar='out_d',
                        help=('create content under output_dir'))

    parser.add_argument('--sign', action='store_true', default=False,
                        help='sign all generated files')

    args = parser.parse_args()

    tree = load_query(args.query_tree, cloud="aws")

    layout = [
        {"name": "stream"},
        {"name": "cloud"},
        {"name": "region"},
        {"name": "build"},
        {"name": "release", "populate": name2ver},
        {"name": "arch"},
    ]

    passthrough = {'output_d': args.out_d,
        'template': STREAM_TEMPLATE.lstrip(),
        'streams': [],
        }
    process(tree, {}, 0, layout, writer, passthrough)

    collections = {}
    for url in passthrough['streams']:
        sys.stderr.write(url + "\n")
        stream = load_content("%s/%s" % (args.out_d, url))
        ctok = ""
        for ptok in [""] + url.split("/")[:-1]:
            ctok += "%s/" % ptok
            if ctok not in collections:
                collections[ctok] = {'streams': []}
                collections[ctok]['tags'] = stream['tags'].copy()
            else:
                clear = []
                for key, val in collections[ctok]['tags'].iteritems():
                    if key not in stream['tags'] or stream['tags'][key] != val:
                        clear.append(key)
                for key in clear:
                    del collections[ctok]['tags'][key]

            collections[ctok]['streams'].append(
                {'tags': stream['tags'].copy(), 'url': url[len(ctok) - 1:]})

    coll_tmpl = COLLECTION_TEMPLATE.lstrip()
    for coll in collections:
        coll_file = "%s%sstreams%s" % (args.out_d, coll, CONTENT_EXT)

        for stream in collections[coll]['streams']:
            for coll_tag in collections[coll]['tags']:
                if coll_tag in stream['tags']:
                    del stream['tags'][coll_tag]

        sys.stderr.write("%s\n" % coll_file)
        with open(coll_file, "w") as fp:
            fp.write(render_string(coll_tmpl, collections[coll]))

        if args.sign:
            signfile(coll_file)

    return

if __name__ == '__main__':
    sys.exit(main())

# vi: ts=4 expandtab
