#!/usr/bin/python

import argparse
import json
import os
import os.path
import requests
import sys
from simplestreams import util

import toolutil

# could support reading from other mirrors
# for example:
#   http://cloud-images-archive.ubuntu.com/
#   file:///srv/ec2-images
#
BASE_URLS = ("http://cloud-images.ubuntu.com/",)

KNOWN_HASHES = {
    10485860: 'c23ea79b857b91a7ff07c6ecf185f1ca',
    10485960: '184cf91ae405aeabad4c49012f190494',
    10486060: '07d250277f50c93c1e8dd14a156e7c06',
}

REAL_DATA = os.environ.get("REAL_DATA", False)
if REAL_DATA and REAL_DATA != "0":
    REAL_DATA = True
else:
    REAL_DATA = False

FILE_DATA = {}


def readurl(url):
    sys.stderr.write("reading %s\n" % url)
    return util.url_reader(url).read()


def get_cache_data(path, field):
    dirname = os.path.dirname(path)
    bname = os.path.basename(path)
    return FILE_DATA.get(dirname, {}).get(bname, {}).get(field)


def store_cache_data(path, field, value):
    dirname = os.path.dirname(path)
    bname = os.path.basename(path)
    if dirname not in FILE_DATA:
        FILE_DATA[dirname] = {}
    if bname not in FILE_DATA[dirname]:
        FILE_DATA[dirname][bname] = {}
    FILE_DATA[dirname][bname][field] = value


def save_cache():
    if FILE_DATA:
        hashcache = FILE_DATA['filename']
        with open(hashcache, "w") as hfp:
            hfp.write(json.dumps(FILE_DATA, indent=1))


def get_cloud_images_file_hash(path):
    md5 = get_cache_data(path, 'md5')
    sha256 = get_cache_data(path, 'sha256')
    if md5 and sha256:
        return {'md5': md5, 'sha256': sha256}

    found = {}
    dirname = os.path.dirname(path)
    for cksum in ("md5", "sha256"):
        content = None
        for burl in BASE_URLS:
            dir_url = burl + dirname

            try:
                content = readurl(dir_url + "/%sSUMS" % cksum.upper())
                break
            except requests.HTTPError as error:
                pass

        if not content:
            raise error

        for line in content.splitlines():
            (hexsum, fname) = line.split()
            if fname.startswith("*"):
                fname = fname[1:]
            found[cksum] = hexsum
            store_cache_data(dirname + "/" + fname, cksum, hexsum)

    md5 = get_cache_data(path, 'md5')
    sha256 = get_cache_data(path, 'sha256')
    save_cache()
    return {'md5': md5, 'sha256': sha256}


def get_url_len(url):
    if url.startswith("file:///"):
        path = url[len("file://"):]
        return os.stat(path).st_size
    if os.path.exists(url):
        return os.stat(url).st_size

    sys.stderr.write("getting size for %s\n" % url)
    request = requests.head(url)
    save_cache()
    return int(request.headers.get('content-length', 0))


def get_cloud_images_file_size(path):
    size = get_cache_data(path, 'size')
    if size:
        return size

    for burl in BASE_URLS:
        try:
            size = int(get_url_len(burl + path))
            break
        except requests.HTTPError as error:
            pass

    if not size:
        raise error
    store_cache_data(path, 'size', size)
    return size


def create_fake_file(prefix, item):
    fpath = os.path.join(prefix, item['path'])
    path = item['path']

    size = 1024
    if path.endswith("-disk1.img"):
        size = 1024 * 1024 * 10 + 100
    elif path.endswith("-root.tar.gz"):
        size = 1024 * 1024 * 10 + 200
    elif path.endswith(".tar.gz"):
        size = 1024 * 1024 * 10 + 300

    toolutil.mkdir_p(os.path.dirname(fpath))
    print "creating %s" % fpath
    with open(fpath, "w") as fp:
        fp.truncate(size)

    item['md5'] = KNOWN_HASHES[size]
    if 'sha256' in item:
        del item['sha256']
    item['size'] = size

    return


def dl_load_query(path):
    tree = {}
    for rline in toolutil.load_query_download(path):
        (stream, rel, build, label, serial, arch, filepath, name) = rline

        if stream not in tree:
            tree[stream] = {'products': {}}
        products = tree[stream]['products']

        prodname = "com.ubuntu.cloud:%s:%s:%s" % (build, rel, arch)

        if prodname not in products:
            products[prodname] = {
                "release": rel,
                "version": toolutil.REL2VER[rel]['version'],
                "arch": arch,
                "versions": {}
            }

        product = products[prodname]

        if serial not in product['versions']:
            product['versions'][serial] = {'items': [], "label": label}
        items = product['versions'][serial]['items']

        items.append({
            'path': filepath,
            'name': name
        })

    return tree


def pubname(label, rel, arch, serial, build='server'):
    version = toolutil.REL2VER[rel]['version']

    if label == "daily":
        rv_label = rel + "-daily"
    elif label == "release":
        rv_label = "%s-%s" % (rel, version)
    elif label.startswith("beta"):
        rv_label = "%s-%s-%s" % (rel, version, label)
    else:
        rv_label = "%s-%s" % (rel, label)
    return "ubuntu-%s-%s-%s-%s" % (rv_label, arch, build, serial)


def ec2_load_query(path):
    tree = {}
    for rline in toolutil.load_query_ec2(path):
        (stream, rel, build, label, serial, store, arch, region,
         iid, _kern, _rmd, vtype) = rline

        if stream not in tree:
            tree[stream] = {'products': {}}
        products = tree[stream]['products']

        prodname = "com.ubuntu.cloud:%s:%s:%s" % (build, rel, arch)

        if prodname not in products:
            products[prodname] = {
                "release": rel,
                "version": toolutil.REL2VER[rel]['version'],
                "arch": arch,
                "versions": {}
            }

        product = products[prodname]

        if serial not in product['versions']:
            product['versions'][serial] = {'items': [], "label": label}
        items = product['versions'][serial]['items']

        name = pubname(label, rel, arch, serial, build)
        product['versions'][serial]['name'] = name

        if store == "instance-store":
            store = 'instance'
        if vtype == "paravirtual":
            vtype = "pv"

        items.append({
            'id': iid,
            'root_store': store,
            'virt': vtype,
            'cloud': region,
        })
    return tree


def printitem(item, exdata):
    full = exdata.copy()
    full.update(item)
    print full

def create_image_data(query_tree, out_d):
    hashcache = os.path.join(query_tree, "FILE_DATA_CACHE")
    FILE_DATA['filename'] = hashcache
    if os.path.isfile(hashcache):
        FILE_DATA.update(json.loads(open(hashcache).read()))

    tree = dl_load_query(query_tree)

    def update_hashes(item, exdata):
        item.update(get_cloud_images_file_hash(item['path']))

    def update_sizes(item, exdata):
        item.update({'size': get_cloud_images_file_size(item['path'])})

    def create_file(item, exdata):
        create_fake_file(out_d, item)

    for stream in tree:
        if REAL_DATA:
            util.walk_items(tree[stream], update_hashes)
            util.walk_items(tree[stream], update_sizes)
        else:
            util.walk_items(tree[stream], create_file)

        tree[stream]['format'] = "products:1.0"

        outfile = os.path.join(out_d, stream, 'dl.js')
        util.mkdir_p(os.path.dirname(outfile))
        with open(outfile, "w") as fp:
            sys.stderr.write("writing %s\n" % outfile)
            fp.write(json.dumps(tree[stream], indent=1) + "\n")

    # save hashes data
    save_cache()
    return tree


def create_aws_data(query_tree, out_d):
    tree = ec2_load_query(query_tree)
    for stream in tree:
        # now add the '_alias' data
        regions = set()
        def findregions(item, exdata):
            regions.add(item['cloud'])
        util.walk_items(tree[stream], findregions)

        tree[stream]['_aliases'] = {'cloud': {}}
        for region in regions:
            tree[stream]['_aliases']['cloud'][region] = {
                'endpoint': 'http://ec2.%s.amazonaws.com' % region,
                'region': region}

        tree[stream]['format'] = "products:1.0"
        outfile = os.path.join(out_d, stream, 'aws.js')
        util.mkdir_p(os.path.dirname(outfile))
        with open(outfile, "w") as fp:
            sys.stderr.write("writing %s\n" % outfile)
            fp.write(json.dumps(tree[stream], indent=1) + "\n")

    return tree


def main():
    parser = argparse.ArgumentParser(description="create example content tree")

    parser.add_argument("query_tree", metavar='query_tree',
                        help=('read in content from /query tree. Hint: ' +
                              'make exdata-query'))

    parser.add_argument("out_d", metavar='out_d',
                        help=('create content under output_dir'))

    parser.add_argument('--sign', action='store_true', default=False,
                        help='sign all generated files')

    args = parser.parse_args()

    dltree = create_image_data(args.query_tree, args.out_d)

    aws_tree = create_aws_data(args.query_tree, args.out_d)

    indexes = {stream:{"index": [], 'format': 'collection:1.0'}
               for stream in aws_tree}

    for streamname in aws_tree:
        index = indexes[streamname]['index']

        clouds = aws_tree[streamname]['_aliases']['cloud'].values()
        index.append({
            'datatype': 'image-ids',
            'clouds': clouds,
            'cloudname': "aws",
            'path': "aws.js",
            'products': aws_tree[streamname]['products'].keys(),
        })
        index.append({
            'datatype': 'image-downloads',
            'path': "dl.js",
            'products': dltree[streamname]['products'].keys()
            })

        outfile = os.path.join(args.out_d, streamname, 'index.js')
        util.mkdir_p(os.path.dirname(outfile))
        with open(outfile, "w") as fp:
            sys.stderr.write("writing %s\n" % outfile)
            fp.write(json.dumps(indexes[streamname], indent=1) + "\n")

    if args.sign:
        def printstatus(name, fmt):
            sys.stderr.write("signing %s: %s\n" % (name, fmt))
        for root, dirs, files in os.walk(args.out_d):
            for f in [f for f in files if f.endswith(".js")]:
                toolutil.signjs_file(os.path.join(root, f), status_cb=printstatus)

#for streamname in STREAMS:
#    aws_stream = copy.deepcopy(aws[streamname])
#    expand_data(aws_stream, aws_stream[ALIASNAME])
#
#    index = indexes[streamname]['index']
#    endpoints = set()
#    products = set()
#    for (pname, product) in aws_stream['products'].iteritems():
#        products.add(pname)
#        for v in product['versions'].values():
#            for i in v['items']:
#                endpoints.add((i['endpoint'], i['region']))
#    clouds = []
#    for (e, r) in endpoints:
#        clouds.append({'endpoint': e, 'region': r})
#
#    index.append({
#        'datatype': 'image-ids',
#        'clouds': clouds,
#        'cloudname': "aws",
#        'path': "aws.js",
#        'products': list(products),
#        })
#
#    index.append({
#        'datatype': 'image-downloads',
#        'path': "dl.js",
#        'products': dl[streamname]['products'].keys()
#        })

    return

if __name__ == '__main__':
    sys.exit(main())

# vi: ts=4 expandtab
