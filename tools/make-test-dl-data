#!/usr/bin/python

import argparse
import copy
import errno
from Cheetah.Template import Template
import json
import os
import os.path
import StringIO
import subprocess
import sys
import yaml
import urllib2

import toolutil

CONTENT_EXT = ".yaml"
#CONTENT_EXT = ".json"

GET_QUERY = ("rsync -avz --delete --exclude \".bzr/*\" " +
             "cloud-images.ubuntu.com::uec-images/query/ ci-query")

RELEASES = (
#    "hardy",
    "lucid",
    "oneiric",
    "precise",
    "quantal",
    "raring",
)

# whitelist of builds
BUILDS = ("server")

KNOWN_HASHES = {
    10485860: 'c23ea79b857b91a7ff07c6ecf185f1ca',
    10485960: '184cf91ae405aeabad4c49012f190494',
    10486060: '07d250277f50c93c1e8dd14a156e7c06',
}

DATA_IN = {
 "streams": ["release", "daily"],
 "releases": [
   {"name": "hardy", "version": "8.04"},
   {"name": "lucid", "version": "10.04"},
   {"name": "oneiric", "version": "11.10"},
   {"name": "precise", "version": "12.04"},
   {"name": "quantal", "version": "12.10"},
   {"name": "raring", "version": "13.04"},
 ],
 "serials": [
   "20121208",
   "20121209.1",
   "20121210",
 ],
 "arches": ["amd64", "i386", "armhf"],
 "flabels": ["-disk1.img", ".tar.gz", "-root.tar.gz"],
}

STREAM_TEMPLATE = """
# required
format: stream-1.0
# recommended
iqn: $iqn
description: Ubuntu Cloud Images

item_groups:
#for $item_group in $item_groups
 - serial: ${item_group.serial}
   label: ${item_group.label}
   items:
   #for $item in $item_group.items
   #set file = $utils.create_file($prefix, $item.path)
    - name: ${item.name}
      md5: ${file.md5}
      #if $file.sha256
      sha256: ${file.sha256}
      #end if
      path: $item.path
   #end for
#end for

tags:
  arch: ${arch.name}
  release: ${release.name}
  version: "${release.version}"
  stream: ${stream.name}
  build: ${build.name}
"""

COLLECTION_TEMPLATE = """
description: Ubuntu Cloud Image streams
format: stream-collection:1.0

streams:
#for $stream in $streams:
 - path: $stream.path
 #for $tag in $stream.tags:
   $tag: $stream.tags[$tag]
 #end for
#end for

tags:
 #for tag in $tags:
 $tag: ${tags[tag]}
 #end for
"""

PGP_SIGNED_MESSAGE_HEADER = "-----BEGIN PGP SIGNED MESSAGE-----"
PGP_SIGNATURE_HEADER = "-----BEGIN PGP SIGNATURE-----"
PGP_SIGNATURE_FOOTER = "-----END PGP SIGNATURE-----"


if bool(os.environ.get("REAL_DATA", False)) is True:
    CREATE_FILE_REAL_HASHES = True
    CREATE_FILE_CREATE = False
else:
    CREATE_FILE_REAL_HASHES = False
    CREATE_FILE_CREATE = True

CREATE_FILE_DATA = {}

def is_expected(repl, fields):
    rel = fields[0]
    serial = fields[3]
    if repl == "-root.tar.gz":
        if rel in ("lucid", "oneiric"):
            # lucid, oneiric do not have -root.tar.gz
            return False
        if rel == "precise" and cmp(serial, "20120202") <= 0:
            # precise got -root.tar.gz after alpha2
            return False

    if repl == "-disk1.img":
        if rel == "lucid":
            return False
        if rel == "oneiric" and cmp(serial, "20110802.2") <= 0:
            # oneiric got -disk1.img after alpha3
            return False

    #if some data in /query is not truely available, fill up this array
    #to skip it.
    #broken = ("precise/20130118.1", "lucid/20130121", "raring/20130125")
    broken = ()
    if "%s/%s" % (rel, serial) in broken:
        print "Known broken: %s/%s" % (rel, serial)
        return False

    return True
                        

def readurl(url):
    sys.stderr.write("reading %s\n" % url)
    return urllib2.urlopen(url).read()

def get_cloud_images_file_hash(path, algo="md5"):
    dirname = os.path.dirname(path)
    bname = os.path.basename(path)
    if dirname in CREATE_FILE_DATA:
        return CREATE_FILE_DATA[dirname][bname]

    CREATE_FILE_DATA[dirname] = {}
    base_url = "http://cloud-images.ubuntu.com/%s" % dirname

    # http://cloud-images.ubuntu.com/server/releases/precise/beta-2/MD5SUMS
    # was giving 403 due to bad redirect, so tried both links.  However
    # seems to work fine now
    # burls = ("http://cloud-images.ubuntu.com",
    #         "http://cloud-images-archive.ubuntu.com")
    burls = ("http://cloud-images.ubuntu.com")
    for cksum in ("md5", "sha256"):
        content = None
        for burl in burls:
            try:
                content = readurl(base_url + "/%sSUMS" % cksum.upper())
                break
            except urllib2.HTTPError as error:
                pass

        if not content:
            raise error

        for line in content.splitlines():
            (hexsum, fname) = line.split()
            if fname.startswith("*"):
                fname = fname[1:]
            if fname not in CREATE_FILE_DATA[dirname]:
                CREATE_FILE_DATA[dirname][fname] = {}
            CREATE_FILE_DATA[dirname][fname][cksum] = hexsum

    return CREATE_FILE_DATA[dirname][bname]

def create_file(prefix, path):
    fpath = os.path.join(prefix, path)
    toolutil.mkdir_p(os.path.dirname(fpath))

    data = {'name': path}
    size = 1024
    if path.endswith("-disk1.img"):
        size = 1024 * 1024 * 10 + 100
    elif path.endswith("-root.tar.gz"):
        size = 1024 * 1024 * 10 + 200
    elif path.endswith(".tar.gz"):
        size = 1024 * 1024 * 10 + 300

    if CREATE_FILE_CREATE:
        with open(fpath, "w") as fp:
            fp.truncate(size)

    if CREATE_FILE_REAL_HASHES:
        data.update(get_cloud_images_file_hash(path))
    else:
        data['md5'] = KNOWN_HASHES[size]
        data['sha256'] = None

    return data
        

def load_query(path, tree=None):
    streams = [f[0:-len(".latest.txt")]
               for f in os.listdir(path)
                   if f.endswith("latest.txt")]
    if tree is None:
        tree = {}

    for stream in streams:
        if stream not in tree:
            tree[stream] = {}
        dl_files = []

        latest_f = "%s/%s.latest.txt" % (path, stream)

        # get the builds and releases
        with open(latest_f) as fp:
            for line in fp.readlines():
                (rel, build, _stream, _serial) = line.split("\t")

                if ((len(BUILDS) and build not in BUILDS) or
                    (len(RELEASES) and rel not in RELEASES)):
                    continue

                dl_files.append("%s/%s/%s/%s-dl.txt" %
                    (path, rel, build, stream))

        field_path = 5
        field_name = 6
        field_release = 0
        # stream/build/release/arch
        cdata = tree[stream]
        for dl_file in dl_files:
            olines = open(dl_file).readlines()

            # download files in /query only contain '.tar.gz' (uec tarball)
            # file.  So we have to make up other entries.
            lines = []
            for oline in olines:
                for repl in (".tar.gz", "-root.tar.gz", "-disk1.img"):
                    fields = oline.rstrip().split("\t")
                    if not is_expected(repl, fields):
                        continue

                    new_path = fields[field_path].replace(".tar.gz", repl)

                    fields[field_path] = new_path
                    fields[field_name] += repl
                    lines.append("\t".join(fields) + "\n")

            for line in lines:
                line = line.rstrip("\n\r") + "\tBOGUS"

                (rel, build, label, serial, arch, filepath, name, _bogus) = \
                    line.split("\t", 8)

                if build not in cdata:
                    cdata[build] = {}
                if rel not in cdata[build]:
                    cdata[build][rel] = {}
                if arch not in cdata[build][rel]:
                    cdata[build][rel][arch] = []

                item_groups = cdata[build][rel][arch]

                item_group = None
                for ig in item_groups:
                    if ig['serial'] == serial:
                        item_group = ig

                if not item_group:
                    item_group = {'serial': serial, 'items': []}
                    item_groups.append(item_group)

                # item groups (same serial) all have the same label
                item_group['label'] = label
                cur_item = {'path': filepath, 'name': name}

                # there were some duplicate rows in /query -dl.txt files.
                if cur_item not in item_group['items']:
                    item_group['items'].append(cur_item)

                # highest serial is first entry in list
                item_groups.sort(reverse=True,
                                 cmp=lambda x,y: cmp(x["serial"], y["serial"]))

    return tree


def name2ver(indict):
    indict.update(toolutil.REL2VER[indict['name']])


def streamwriter(item_groups, data, path, passthrough):
    iqn = ("iqn.2005-04.com.ubuntu:cloud-images:%s:%s:%s:%s:stream" %
           (data['stream']['name'], data['release']['name'], 
            data['build']['name'], data['arch']['name']))
    data['iqn'] = iqn

    params = data.copy()
    params['item_groups'] = item_groups
    params.update(passthrough.get('params', {}))
    url = "%s/%s.yaml" % (data['build']['name'], path)
    fname = "%s/%s" % (passthrough['output_d'], url)
    toolutil.mkdir_p(os.path.dirname(fname))
    with open(fname, "w") as fp:
        fp.write(toolutil.render_string(
            passthrough['template'], params))


    for igroup in item_groups:
        for item in igroup['items']:
            ipath = item.get("path")
            if ipath:
                if ipath not in passthrough['data_files']:
                    passthrough['data_files'].append(ipath)

    if passthrough['sign']:
        toolutil.signfile(fname)

    passthrough['streams'].append(url)
    sys.stderr.write(fname + "\n")


def main():
    parser = argparse.ArgumentParser(description="create example content tree")

    parser.add_argument("query_tree", metavar='query_tree',
                        help=('read in content from /query tree. Hint: ' +
                              GET_QUERY))

    parser.add_argument("out_d", metavar='out_d',
                        help=('create content under output_dir'))

    parser.add_argument('--sign', action='store_true', default=False,
                        help='sign all generated files')

    args = parser.parse_args()

    tree = load_query(args.query_tree)

    # stream/build/release/arch
    layout = [
        {"name": "stream"},
        {"name": "build"},
        {"name": "release", "populate": name2ver},
        {"name": "arch"},
    ]

    passthrough = {'output_d': args.out_d,
        'template': STREAM_TEMPLATE.lstrip(),
        'streams': [],
        'data_files': [],
        'params': { 
            'utils': { 'create_file': create_file },
            'prefix': args.out_d,
        },
        'sign': args.sign,
    }

    global CREATE_FILE_DATA
    if CREATE_FILE_REAL_HASHES:
        hashcache = os.path.join(args.query_tree, "HASH_CACHE")
        if os.path.isfile(hashcache):
            CREATE_FILE_DATA = yaml.safe_load(open(hashcache, "r"))

    toolutil.process(tree, {}, 0, layout, streamwriter, passthrough)

    tmpl = COLLECTION_TEMPLATE.lstrip()

    def collwriter(path, path_prefix, collection):
        toks = ("stream", "release", "buildname", "arch", "cloud", "region")

        coll_file = "%s%sstreams%s" % (path_prefix, path, CONTENT_EXT)

        toolutil.mkdir_p(os.path.dirname(coll_file))
        with open(coll_file, "w") as cfp:
            cfp.write(toolutil.render_string(tmpl, collection))

        if args.sign:
            toolutil.signfile(coll_file)

    toolutil.process_collections(passthrough['streams'], args.out_d,
                                 collwriter)

    with open(os.path.join(args.out_d, "MIRROR.info"), "w") as mfp:
        mfp.write(toolutil.dumps({
            'iqn': 'iqn.2005-04.com.ubuntu:cloud-images:download',
            'authoritative_mirror': 'http://cloud-images.ubuntu.com/',
            'mirrors': []
            }))

    # save hashes data
    if CREATE_FILE_REAL_HASHES:
        with open(hashcache, "w") as hfp:
            hfp.write(yaml.safe_dump(CREATE_FILE_DATA))

    return

if __name__ == '__main__':
    sys.exit(main())

# vi: ts=4 expandtab
