#!/usr/bin/python

import argparse
import copy
import errno
from Cheetah.Template import Template
import json
import os
import os.path
import StringIO
import subprocess
import sys
import yaml

import toolutil

CONTENT_EXT = ".yaml"
#CONTENT_EXT = ".json"

GET_QUERY = ("rsync -avz --delete --exclude \".bzr/*\" " +
             "cloud-images.ubuntu.com::uec-images/query/ ci-query")

RELEASES = (
#    "hardy",
    "lucid",
    "oneiric",
    "precise",
    "quantal",
    "raring",
)

# whitelist of builds
BUILDS = ("server")

KNOWN_HASHES = {
    10485860: 'c23ea79b857b91a7ff07c6ecf185f1ca',
    10485960: '184cf91ae405aeabad4c49012f190494',
    10486060: '07d250277f50c93c1e8dd14a156e7c06',
}

DATA_IN = {
 "streams": ["release", "daily"],
 "releases": [
   {"name": "hardy", "version": "8.04"},
   {"name": "lucid", "version": "10.04"},
   {"name": "oneiric", "version": "11.10"},
   {"name": "precise", "version": "12.04"},
   {"name": "quantal", "version": "12.10"},
   {"name": "raring", "version": "13.04"},
 ],
 "serials": [
   "20121208",
   "20121209.1",
   "20121210",
 ],
 "arches": ["amd64", "i386", "armhf"],
 "flabels": ["-disk1.img", ".tar.gz", "-root.tar.gz"],
}

STREAM_TEMPLATE = """
# required
format: stream-1.0
# recommended
description: Ubuntu Cloud Images

item_groups:
#for $item_group in $item_groups
 - serial: ${item_group.serial}
   label: ${item_group.label}
   items:
   #for $item in $item_group.items
   #set file = $utils.create_file($prefix, $item.path)
    - name: ${item.name}
      md5sum: ${file.md5sum}
      path: $item.path
   #end for
#end for

tags:
  arch: ${arch.name}
  release: ${release.name}
  version: "${release.version}"
  stream: ${stream.name}
  build: ${build.name}
"""

COLLECTION_TEMPLATE = """
description: Ubuntu Cloud Image streams
format: stream-collection:1.0

streams:
#for $stream in $streams:
 - path: $stream.path
 #for $tag in $stream.tags:
   $tag: $stream.tags[$tag]
 #end for
#end for

tags:
 #for tag in $tags:
 $tag: ${tags[tag]}
 #end for
"""

PGP_SIGNED_MESSAGE_HEADER = "-----BEGIN PGP SIGNED MESSAGE-----"
PGP_SIGNATURE_HEADER = "-----BEGIN PGP SIGNATURE-----"
PGP_SIGNATURE_FOOTER = "-----END PGP SIGNATURE-----"


def create_file(prefix, path):
    fpath = os.path.join(prefix, path)
    toolutil.mkdir_p(os.path.dirname(fpath))
    size = 1024
    if path.endswith("-disk1.img"):
        size = 1024 * 1024 * 10 + 100
    elif path.endswith("-root.tar.gz"):
        size = 1024 * 1024 * 10 + 200
    elif path.endswith(".tar.gz"):
        size = 1024 * 1024 * 10 + 300
    with open(fpath, "w") as fp:
        fp.truncate(size)

    return({'name': path, 'md5sum': KNOWN_HASHES[size]})
        

def load_query(path, tree=None):
    streams = [f[0:-len(".latest.txt")]
               for f in os.listdir(path)
                   if f.endswith("latest.txt")]
    if tree is None:
        tree = {}

    for stream in streams:
        if stream not in tree:
            tree[stream] = {}
        dl_files = []

        latest_f = "%s/%s.latest.txt" % (path, stream)

        # get the builds and releases
        with open(latest_f) as fp:
            for line in fp.readlines():
                (rel, build, _stream, _serial) = line.split("\t")

                if ((len(BUILDS) and build not in BUILDS) or
                    (len(RELEASES) and rel not in RELEASES)):
                    continue

                dl_files.append("%s/%s/%s/%s-dl.txt" %
                    (path, rel, build, stream))

        field_path = 5
        field_name = 6
        # stream/build/release/arch
        cdata = tree[stream]
        for dl_file in dl_files:
            olines = open(dl_file).readlines()

            # download files in /query only contain '.tar.gz' (uec tarball)
            # file.  So we have to make up other entries.
            lines = []
            for oline in olines:
                for repl in (".tar.gz", "-root.tar.gz", "-disk1.img"):
                    fields = oline.rstrip().split("\t")
                    new_path = fields[field_path].replace(".tar.gz", repl)
                    fields[field_path] = new_path
                    fields[field_name] += repl
                    lines.append("\t".join(fields) + "\n")

            for line in lines:
                line = line.rstrip("\n\r") + "\tBOGUS"

                (rel, build, label, serial, arch, filepath, name, _bogus) = \
                    line.split("\t", 8)

                if build not in cdata:
                    cdata[build] = {}
                if rel not in cdata[build]:
                    cdata[build][rel] = {}
                if arch not in cdata[build][rel]:
                    cdata[build][rel][arch] = []

                item_groups = cdata[build][rel][arch]

                item_group = None
                for ig in item_groups:
                    if ig['serial'] == serial:
                        item_group = ig

                if not item_group:
                    item_group = {'serial': serial, 'items': []}
                    item_groups.append(item_group)

                # item groups (same serial) all have the same label
                item_group['label'] = label
                cur_item = {'path': filepath, 'name': name}

                # there were some duplicate rows in /query -dl.txt files.
                if cur_item not in item_group['items']:
                    item_group['items'].append(cur_item)

                # highest serial is first entry in list
                item_groups.sort(reverse=True,
                                 cmp=lambda x,y: cmp(x["serial"], y["serial"]))

    return tree


def name2ver(indict):
    indict.update(toolutil.REL2VER[indict['name']])


def writer(item, data, path, passthrough):
    params = data.copy()
    params['item_groups'] = item
    params.update(passthrough.get('params', {}))
    url = "%s.yaml" % path
    fname = "%s/%s" % (passthrough['output_d'], url)
    toolutil.mkdir_p(os.path.dirname(fname))
    with open(fname, "w") as fp:
        fp.write(toolutil.render_string(
            passthrough['template'], params))

    if passthrough['sign']:
        toolutil.signfile(fname)

    passthrough['streams'].append(url)
    sys.stderr.write(fname + "\n")


def main():
    parser = argparse.ArgumentParser(description="create example content tree")

    parser.add_argument("query_tree", metavar='query_tree',
                        help=('read in content from /query tree. Hint: ' +
                              GET_QUERY))

    parser.add_argument("out_d", metavar='out_d',
                        help=('create content under output_dir'))

    parser.add_argument('--sign', action='store_true', default=False,
                        help='sign all generated files')

    args = parser.parse_args()

    tree = load_query(args.query_tree)

    # stream/build/release/arch
    layout = [
        {"name": "stream"},
        {"name": "build"},
        {"name": "release", "populate": name2ver},
        {"name": "arch"},
    ]

    passthrough = {'output_d': args.out_d,
        'template': STREAM_TEMPLATE.lstrip(),
        'streams': [],
        'params': { 
            'utils': { 'create_file': create_file },
            'prefix': args.out_d,
        },
        'sign': args.sign,
    }

    try:
        maps = toolutil.load_content("%s/MAP" % args.out_d)
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
        maps = {}
    if 'files' not in maps:
        maps['files'] = {}

    passthrough['maps'] = maps

    toolutil.process(tree, {}, 0, layout, writer, passthrough)

    tmpl = COLLECTION_TEMPLATE.lstrip()

    def collwriter(path, path_prefix, collection):
        toks = ("stream", "release", "buildname", "arch", "cloud", "region")

        coll_file = "%s%sstreams%s" % (path_prefix, path, CONTENT_EXT)

        toolutil.mkdir_p(os.path.dirname(coll_file))
        with open(coll_file, "w") as cfp:
            cfp.write(toolutil.render_string(tmpl, collection))

        if args.sign:
            toolutil.signfile(coll_file)

    toolutil.process_collections(passthrough['streams'], args.out_d,
                                 collwriter)

    with open(os.path.join(args.out_d, "MAP"), "w") as mfp:
        mfp.write(toolutil.dumps(passthrough['maps']))

    return

if __name__ == '__main__':
    sys.exit(main())

# vi: ts=4 expandtab
